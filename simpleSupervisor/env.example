# Ollama Configuration
# Make sure Ollama is running locally with llama3.2 model
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2

# Optional: Customize model settings
# OLLAMA_TEMPERATURE=0.7
# OLLAMA_TIMEOUT=30 